{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn  import functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_solve(z0, U, t0, t1, f):\n",
    "    \"\"\"\n",
    "    Simplest Euler ODE initial value solver\n",
    "    \"\"\"\n",
    "    h_max = 0.05\n",
    "    n_steps = math.ceil((abs(t1 - t0)/h_max).max().item())\n",
    "\n",
    "    h = (t1 - t0)/n_steps\n",
    "    t = t0\n",
    "    z = z0\n",
    "\n",
    "    for i_step in range(n_steps):\n",
    "        z = z + h * f(z, U, t)\n",
    "        t = t + h\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the below cell for forward and backward may need to be rewrite\n",
    "class ODEAdjoint(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, z0, U, t, flat_parameters, func):\n",
    "        assert isinstance(func, ODEF)\n",
    "        bs, *z_shape = z0.size()\n",
    "        time_len = t.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = torch.zeros(time_len, bs, *z_shape).to(z0)\n",
    "            z[0] = z0\n",
    "            for i_t in range(time_len - 1):\n",
    "                z0 = ode_solve(z0, U, t[i_t], t[i_t+1], func)\n",
    "                z[i_t+1] = z0\n",
    "\n",
    "        ctx.func = func\n",
    "        ctx.save_for_backward(t, U, z.clone(), flat_parameters)\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dLdz):\n",
    "        \"\"\"\n",
    "        dLdz shape: time_len, batch_size, *z_shape\n",
    "        \"\"\"\n",
    "        func = ctx.func\n",
    "        t, U, z, flat_parameters = ctx.saved_tensors\n",
    "        time_len, bs, *z_shape = z.size()\n",
    "        n_dim = np.prod(z_shape)\n",
    "        n_params = flat_parameters.size(0)\n",
    "\n",
    "        # Dynamics of augmented system to be calculated backwards in time\n",
    "        def augmented_dynamics(aug_z_i, t_i):\n",
    "            \"\"\"\n",
    "            tensors here are temporal slices\n",
    "            t_i - is tensor with size: bs, 1\n",
    "            aug_z_i - is tensor with size: bs, n_dim*2 + n_params + 1\n",
    "            \"\"\"\n",
    "            z_i, a = aug_z_i[:, :n_dim], aug_z_i[:, n_dim:2*n_dim]  # ignore parameters and time\n",
    "\n",
    "            # Unflatten z and a\n",
    "            z_i = z_i.view(bs, *z_shape)\n",
    "            a = a.view(bs, *z_shape)\n",
    "            with torch.set_grad_enabled(True):\n",
    "                t_i = t_i.detach().requires_grad_(True)\n",
    "                z_i = z_i.detach().requires_grad_(True)\n",
    "                U_i = U.detach().requires_grad_(True)\n",
    "                func_eval, adfdz, adfdt, adfdp = func.forward_with_grad(z_i, U_i, t_i, grad_outputs=a)  # bs, *z_shape\n",
    "                adfdz = adfdz.to(z_i) if adfdz is not None else torch.zeros(bs, *z_shape).to(z_i)\n",
    "                adfdp = adfdp.to(z_i) if adfdp is not None else torch.zeros(bs, n_params).to(z_i)\n",
    "                adfdt = adfdt.to(z_i) if adfdt is not None else torch.zeros(bs, 1).to(z_i)\n",
    "\n",
    "            # Flatten f and adfdz\n",
    "            func_eval = func_eval.view(bs, n_dim)\n",
    "            adfdz = adfdz.view(bs, n_dim)\n",
    "            return torch.cat((func_eval, -adfdz, -adfdp, -adfdt), dim=1)\n",
    "\n",
    "        dLdz = dLdz.view(time_len, bs, n_dim)  # flatten dLdz for convenience\n",
    "        with torch.no_grad():\n",
    "            ## Create placeholders for output gradients\n",
    "            # Prev computed backwards adjoints to be adjusted by direct gradients\n",
    "            adj_z = torch.zeros(bs, n_dim).to(dLdz)\n",
    "            adj_p = torch.zeros(bs, n_params).to(dLdz)\n",
    "            # In contrast to z and p we need to return gradients for all times\n",
    "            adj_t = torch.zeros(time_len, bs, 1).to(dLdz)\n",
    "\n",
    "            for i_t in range(time_len-1, 0, -1):\n",
    "                z_i = z[i_t]\n",
    "                t_i = t[i_t]\n",
    "                f_i = func(z_i, t_i).view(bs, n_dim)\n",
    "\n",
    "                # Compute direct gradients\n",
    "                dLdz_i = dLdz[i_t]\n",
    "                dLdt_i = torch.bmm(torch.transpose(dLdz_i.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0]\n",
    "\n",
    "                # Adjusting adjoints with direct gradients\n",
    "                adj_z += dLdz_i\n",
    "                adj_t[i_t] = adj_t[i_t] - dLdt_i\n",
    "\n",
    "                # Pack augmented variable\n",
    "                aug_z = torch.cat((z_i.view(bs, n_dim), adj_z, torch.zeros(bs, n_params).to(z), adj_t[i_t]), dim=-1)\n",
    "\n",
    "                # Solve augmented system backwards\n",
    "                aug_ans = ode_solve(aug_z, t_i, t[i_t-1], augmented_dynamics)\n",
    "\n",
    "                # Unpack solved backwards augmented system\n",
    "                adj_z[:] = aug_ans[:, n_dim:2*n_dim]\n",
    "                adj_p[:] += aug_ans[:, 2*n_dim:2*n_dim + n_params]\n",
    "                adj_t[i_t-1] = aug_ans[:, 2*n_dim + n_params:]\n",
    "\n",
    "                del aug_z, aug_ans\n",
    "\n",
    "            ## Adjust 0 time adjoint with direct gradients\n",
    "            # Compute direct gradients\n",
    "            dLdz_0 = dLdz[0]\n",
    "            dLdt_0 = torch.bmm(torch.transpose(dLdz_0.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0]\n",
    "\n",
    "            # Adjust adjoints\n",
    "            adj_z += dLdz_0\n",
    "            adj_t[0] = adj_t[0] - dLdt_0\n",
    "        return adj_z.view(bs, *z_shape), adj_t, adj_p, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEF(nn.Module):\n",
    "    def forward_with_grad(self, z, U, t, grad_outputs):\n",
    "        \"\"\"Compute f and a df/dz, a df/dp, a df/dt\"\"\"\n",
    "        batch_size = z.shape[0]\n",
    "\n",
    "        out = self.forward(z, U, t)\n",
    "\n",
    "        a = grad_outputs\n",
    "        adfdz, adfdt, *adfdp = torch.autograd.grad(\n",
    "            (out,), (z, U, t) + tuple(self.parameters()), grad_outputs=(a),\n",
    "            allow_unused=True, retain_graph=True\n",
    "        )\n",
    "        # grad method automatically sums gradients for batch items, we have to expand them back\n",
    "        if adfdp is not None:\n",
    "            adfdp = torch.cat([p_grad.flatten() for p_grad in adfdp]).unsqueeze(0)\n",
    "            adfdp = adfdp.expand(batch_size, -1) / batch_size\n",
    "        if adfdt is not None:\n",
    "            adfdt = adfdt.expand(batch_size, 1) / batch_size\n",
    "        return out, adfdz, adfdt, adfdp\n",
    "\n",
    "    def flatten_parameters(self):\n",
    "        p_shapes = []\n",
    "        flat_parameters = []\n",
    "        for p in self.parameters():\n",
    "            p_shapes.append(p.size())\n",
    "            flat_parameters.append(p.flatten())\n",
    "        return p_shapes, flat_parameters\n",
    "\n",
    "class NeuralODE(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super(NeuralODE, self).__init__()\n",
    "        assert isinstance(func, ODEF)\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, z0, U, t=Tensor([0., 1.]), return_whole_sequence=False):\n",
    "        t = t.to(z0)\n",
    "        z = ODEAdjoint.apply(z0, U, t, self.func.flatten_parameters(), self.func)\n",
    "        if return_whole_sequence:\n",
    "            return z\n",
    "        else:\n",
    "            return z[-1]\n",
    "\n",
    "class LinearODEF_STI(ODEF):\n",
    "    def __init__(self, W, C):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(W.shape[0], W.shape[1], bias=False)\n",
    "        self.lin.weight = nn.Parameter(W)\n",
    "        \n",
    "        self.sti = nn.Linear(C.shape[0], C.shape[1], bias=False)\n",
    "        self.sti.weight = nn.Parameter(C)\n",
    "        \n",
    "    def forward(self, x, U, t):\n",
    "        return self.lin(x) + self.sti(U)\n",
    "\n",
    "        \n",
    "class RandomLinearODEF_STI(LinearODEF_STI):\n",
    "    def __init__(self, W, C):\n",
    "        super().__init__(torch.randn(W.shape[0], W.shape[1])/2., torch.randn(C.shape[0], C.shape[1])/2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(x):\n",
    "    return x.detach().cpu().numpy()\n",
    "\n",
    "def conduct_experiment(ode_true, ode_trained, n_steps, epoch=5):\n",
    "    # Create data\n",
    "    z0 = Variable(torch.Tensor([[0.5, 0.3, -0.1]]))\n",
    "\n",
    "    t_max = 6.29*5\n",
    "    n_points = 200\n",
    "\n",
    "    print(f\"Training Epoch {epoch}...\")\n",
    "\n",
    "    index_np = np.arange(0, n_points, 1, dtype=np.int)\n",
    "    index_np = np.hstack([index_np[:, None]])\n",
    "    times_np = np.linspace(0, t_max, num=n_points)\n",
    "    times_np = np.hstack([times_np[:, None]])\n",
    "\n",
    "    times = torch.from_numpy(times_np[:, :, None]).to(z0)\n",
    "    obs = ode_true(z0, U, times, return_whole_sequence=True).detach()\n",
    "    obs = obs + torch.randn_like(obs) * 0.01\n",
    "\n",
    "    # Get trajectory of random timespan\n",
    "    min_delta_time = 1.0\n",
    "    max_delta_time = 5.0\n",
    "    max_points_num = 32\n",
    "    def create_batch():\n",
    "        t0 = np.random.uniform(0, t_max - max_delta_time)\n",
    "        t1 = t0 + np.random.uniform(min_delta_time, max_delta_time)\n",
    "\n",
    "        idx = sorted(np.random.permutation(index_np[(times_np > t0) & (times_np < t1)])[:max_points_num])\n",
    "\n",
    "        obs_ = obs[idx]\n",
    "        ts_ = times[idx]\n",
    "        return obs_, ts_\n",
    "\n",
    "    # Train Neural ODE\n",
    "    optimizer = torch.optim.Adam(ode_trained.parameters(), lr=0.01)\n",
    "    train_losses = []\n",
    "    for i in range(n_steps):\n",
    "        obs_, ts_ = create_batch()\n",
    "        z_ = ode_trained(obs_[0], U, ts_, return_whole_sequence=True)\n",
    "        loss = F.mse_loss(z_, obs_.detach())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    print('Mean Train loss: {:.5f}'.format(np.mean(train_losses)))\n",
    "    print(list(ode_trained.parameters()))\n",
    "    final_pars = list(ode_trained.parameters())\n",
    "    return train_losses, final_pars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1:given addition parameter C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given equation below<br>\n",
    "$\\frac{dz}{dt} =  \\begin{bmatrix}\n",
    "-0.6 & 0.7 & -0.8 \\\\\n",
    "-0.2 & 0.3 & 1.1 \\\\\n",
    "0.2 & -0.5 & -0.5 \n",
    "\\end{bmatrix}z +  \\begin{bmatrix}\n",
    "-0.4 & 0 \\\\\n",
    "0 & 0 \\\\\n",
    "0 & 0 \n",
    "\\end{bmatrix} * u $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of NeuralODE(\n",
      "  (func): LinearODEF_STI(\n",
      "    (lin): Linear(in_features=3, out_features=3, bias=False)\n",
      "    (sti): Linear(in_features=3, out_features=2, bias=False)\n",
      "  )\n",
      ")>\n",
      "<bound method Module.parameters of NeuralODE(\n",
      "  (func): RandomLinearODEF_STI(\n",
      "    (lin): Linear(in_features=3, out_features=3, bias=False)\n",
      "    (sti): Linear(in_features=3, out_features=2, bias=False)\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "A = Tensor([[-0.6, 0.7, -0.8], [-0.2, 0.3, 1.1], [0.2, -0.5, -0.5]])\n",
    "C = Tensor([[-0.4, 0], [0, 0], [0, 0]])\n",
    "U = Tensor([[0.5, 0]])\n",
    "\n",
    "ode_true = NeuralODE(LinearODEF_STI(A, C))\n",
    "ode_trained = NeuralODE(RandomLinearODEF_STI(A, C))\n",
    "\n",
    "print(ode_true.parameters)\n",
    "print(ode_trained.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_np = np.arange(0, n_points, 1, dtype=np.int)\n",
    "index_np = np.hstack([index_np[:, None]])\n",
    "times_np = np.linspace(0, t_max, num=n_points)\n",
    "times_np = np.hstack([times_np[:, None]])\n",
    "\n",
    "times = torch.from_numpy(times_np[:, :, None]).to(z0)\n",
    "obs = ode_true(z0, U, times, return_whole_sequence=True)\n",
    "obs = obs + torch.randn_like(obs) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5062,  0.2966, -0.1073]],\n",
       "\n",
       "        [[ 0.4803,  0.2892, -0.1017]],\n",
       "\n",
       "        [[ 0.4405,  0.2530, -0.1007]],\n",
       "\n",
       "        [[ 0.3884,  0.2383, -0.1042]],\n",
       "\n",
       "        [[ 0.3733,  0.2210, -0.0983]],\n",
       "\n",
       "        [[ 0.3499,  0.2012, -0.0822]],\n",
       "\n",
       "        [[ 0.3351,  0.1849, -0.1084]],\n",
       "\n",
       "        [[ 0.2965,  0.1886, -0.1146]],\n",
       "\n",
       "        [[ 0.2527,  0.1602, -0.0799]],\n",
       "\n",
       "        [[ 0.2508,  0.1572, -0.0806]],\n",
       "\n",
       "        [[ 0.2087,  0.1271, -0.0654]],\n",
       "\n",
       "        [[ 0.1769,  0.1286, -0.0899]],\n",
       "\n",
       "        [[ 0.1434,  0.0973, -0.0824]],\n",
       "\n",
       "        [[ 0.1303,  0.0740, -0.0563]],\n",
       "\n",
       "        [[ 0.0919,  0.0794, -0.0725]],\n",
       "\n",
       "        [[ 0.0773,  0.0502, -0.0751]],\n",
       "\n",
       "        [[ 0.0517,  0.0540, -0.0387]],\n",
       "\n",
       "        [[ 0.0169,  0.0452, -0.0559]],\n",
       "\n",
       "        [[ 0.0015,  0.0422, -0.0475]],\n",
       "\n",
       "        [[-0.0167,  0.0363, -0.0424]],\n",
       "\n",
       "        [[-0.0249,  0.0129, -0.0602]],\n",
       "\n",
       "        [[-0.0497,  0.0032, -0.0495]],\n",
       "\n",
       "        [[-0.0687, -0.0025, -0.0358]],\n",
       "\n",
       "        [[-0.0790,  0.0032, -0.0542]],\n",
       "\n",
       "        [[-0.1198,  0.0089, -0.0344]],\n",
       "\n",
       "        [[-0.1116, -0.0203, -0.0556]],\n",
       "\n",
       "        [[-0.1414, -0.0199, -0.0430]],\n",
       "\n",
       "        [[-0.1428, -0.0132, -0.0461]],\n",
       "\n",
       "        [[-0.1696, -0.0270, -0.0528]],\n",
       "\n",
       "        [[-0.1814, -0.0338, -0.0364]],\n",
       "\n",
       "        [[-0.2009, -0.0350, -0.0440]],\n",
       "\n",
       "        [[-0.2054, -0.0440, -0.0425]],\n",
       "\n",
       "        [[-0.2200, -0.0404, -0.0491]],\n",
       "\n",
       "        [[-0.2474, -0.0261, -0.0511]],\n",
       "\n",
       "        [[-0.2293, -0.0490, -0.0494]],\n",
       "\n",
       "        [[-0.2352, -0.0330, -0.0499]],\n",
       "\n",
       "        [[-0.2520, -0.0516, -0.0449]],\n",
       "\n",
       "        [[-0.2586, -0.0573, -0.0455]],\n",
       "\n",
       "        [[-0.2653, -0.0679, -0.0434]],\n",
       "\n",
       "        [[-0.2760, -0.0566, -0.0379]],\n",
       "\n",
       "        [[-0.2977, -0.0536, -0.0627]],\n",
       "\n",
       "        [[-0.2834, -0.0522, -0.0388]],\n",
       "\n",
       "        [[-0.2897, -0.0604, -0.0582]],\n",
       "\n",
       "        [[-0.3095, -0.0676, -0.0336]],\n",
       "\n",
       "        [[-0.3053, -0.0763, -0.0382]],\n",
       "\n",
       "        [[-0.3101, -0.0761, -0.0355]],\n",
       "\n",
       "        [[-0.3221, -0.0700, -0.0341]],\n",
       "\n",
       "        [[-0.2958, -0.0651, -0.0627]],\n",
       "\n",
       "        [[-0.3181, -0.0972, -0.0419]],\n",
       "\n",
       "        [[-0.3024, -0.0628, -0.0695]],\n",
       "\n",
       "        [[-0.3230, -0.0775, -0.0576]],\n",
       "\n",
       "        [[-0.3230, -0.0918, -0.0429]],\n",
       "\n",
       "        [[-0.3293, -0.0857, -0.0556]],\n",
       "\n",
       "        [[-0.3289, -0.0817, -0.0452]],\n",
       "\n",
       "        [[-0.3518, -0.0878, -0.0324]],\n",
       "\n",
       "        [[-0.3446, -0.0815, -0.0540]],\n",
       "\n",
       "        [[-0.3388, -0.0920, -0.0574]],\n",
       "\n",
       "        [[-0.3598, -0.0897, -0.0573]],\n",
       "\n",
       "        [[-0.3359, -0.0980, -0.0506]],\n",
       "\n",
       "        [[-0.3545, -0.0880, -0.0604]],\n",
       "\n",
       "        [[-0.3531, -0.1199, -0.0431]],\n",
       "\n",
       "        [[-0.3634, -0.1172, -0.0363]],\n",
       "\n",
       "        [[-0.3583, -0.1025, -0.0579]],\n",
       "\n",
       "        [[-0.3721, -0.1193, -0.0372]],\n",
       "\n",
       "        [[-0.3696, -0.1146, -0.0466]],\n",
       "\n",
       "        [[-0.3613, -0.1080, -0.0377]],\n",
       "\n",
       "        [[-0.3902, -0.1092, -0.0501]],\n",
       "\n",
       "        [[-0.3764, -0.1258, -0.0613]],\n",
       "\n",
       "        [[-0.3781, -0.1105, -0.0398]],\n",
       "\n",
       "        [[-0.3727, -0.1243, -0.0367]],\n",
       "\n",
       "        [[-0.3845, -0.1078, -0.0596]],\n",
       "\n",
       "        [[-0.3820, -0.1101, -0.0414]],\n",
       "\n",
       "        [[-0.3961, -0.1141, -0.0572]],\n",
       "\n",
       "        [[-0.3690, -0.1063, -0.0311]],\n",
       "\n",
       "        [[-0.3948, -0.1049, -0.0324]],\n",
       "\n",
       "        [[-0.4020, -0.1235, -0.0371]],\n",
       "\n",
       "        [[-0.4106, -0.1030, -0.0527]],\n",
       "\n",
       "        [[-0.4007, -0.1281, -0.0361]],\n",
       "\n",
       "        [[-0.3871, -0.1003, -0.0323]],\n",
       "\n",
       "        [[-0.4254, -0.1061, -0.0585]],\n",
       "\n",
       "        [[-0.3883, -0.1319, -0.0218]],\n",
       "\n",
       "        [[-0.3882, -0.1255, -0.0500]],\n",
       "\n",
       "        [[-0.3778, -0.1243, -0.0450]],\n",
       "\n",
       "        [[-0.4146, -0.1076, -0.0496]],\n",
       "\n",
       "        [[-0.4028, -0.1073, -0.0492]],\n",
       "\n",
       "        [[-0.4029, -0.1164, -0.0488]],\n",
       "\n",
       "        [[-0.4280, -0.1018, -0.0408]],\n",
       "\n",
       "        [[-0.4079, -0.1120, -0.0380]],\n",
       "\n",
       "        [[-0.4105, -0.0961, -0.0440]],\n",
       "\n",
       "        [[-0.4135, -0.1251, -0.0518]],\n",
       "\n",
       "        [[-0.4186, -0.1102, -0.0480]],\n",
       "\n",
       "        [[-0.4168, -0.1164, -0.0421]],\n",
       "\n",
       "        [[-0.4121, -0.1006, -0.0471]],\n",
       "\n",
       "        [[-0.4130, -0.1138, -0.0635]],\n",
       "\n",
       "        [[-0.4250, -0.0997, -0.0452]],\n",
       "\n",
       "        [[-0.4032, -0.1316, -0.0488]],\n",
       "\n",
       "        [[-0.4103, -0.1155, -0.0279]],\n",
       "\n",
       "        [[-0.4164, -0.1224, -0.0335]],\n",
       "\n",
       "        [[-0.4276, -0.1260, -0.0513]],\n",
       "\n",
       "        [[-0.4122, -0.1300, -0.0306]],\n",
       "\n",
       "        [[-0.3941, -0.1249, -0.0430]],\n",
       "\n",
       "        [[-0.4234, -0.1143, -0.0431]],\n",
       "\n",
       "        [[-0.4171, -0.1360, -0.0490]],\n",
       "\n",
       "        [[-0.4242, -0.1075, -0.0443]],\n",
       "\n",
       "        [[-0.4241, -0.1362, -0.0338]],\n",
       "\n",
       "        [[-0.4240, -0.1353, -0.0473]],\n",
       "\n",
       "        [[-0.4148, -0.1263, -0.0331]],\n",
       "\n",
       "        [[-0.4144, -0.1229, -0.0501]],\n",
       "\n",
       "        [[-0.4324, -0.1330, -0.0595]],\n",
       "\n",
       "        [[-0.4141, -0.1236, -0.0570]],\n",
       "\n",
       "        [[-0.4083, -0.1267, -0.0464]],\n",
       "\n",
       "        [[-0.3980, -0.1254, -0.0508]],\n",
       "\n",
       "        [[-0.4152, -0.1276, -0.0456]],\n",
       "\n",
       "        [[-0.4036, -0.1342, -0.0293]],\n",
       "\n",
       "        [[-0.4246, -0.1236, -0.0353]],\n",
       "\n",
       "        [[-0.4072, -0.1187, -0.0481]],\n",
       "\n",
       "        [[-0.4171, -0.1322, -0.0499]],\n",
       "\n",
       "        [[-0.4308, -0.1180, -0.0437]],\n",
       "\n",
       "        [[-0.4392, -0.1236, -0.0288]],\n",
       "\n",
       "        [[-0.4137, -0.1210, -0.0447]],\n",
       "\n",
       "        [[-0.4224, -0.1379, -0.0279]],\n",
       "\n",
       "        [[-0.4211, -0.1459, -0.0390]],\n",
       "\n",
       "        [[-0.4107, -0.1405, -0.0369]],\n",
       "\n",
       "        [[-0.4443, -0.1308, -0.0298]],\n",
       "\n",
       "        [[-0.4401, -0.1249, -0.0346]],\n",
       "\n",
       "        [[-0.4338, -0.1312, -0.0589]],\n",
       "\n",
       "        [[-0.4114, -0.1301, -0.0127]],\n",
       "\n",
       "        [[-0.4305, -0.1210, -0.0396]],\n",
       "\n",
       "        [[-0.4264, -0.1127, -0.0562]],\n",
       "\n",
       "        [[-0.4082, -0.1390, -0.0289]],\n",
       "\n",
       "        [[-0.4313, -0.1118, -0.0548]],\n",
       "\n",
       "        [[-0.4269, -0.1139, -0.0486]],\n",
       "\n",
       "        [[-0.4327, -0.1216, -0.0230]],\n",
       "\n",
       "        [[-0.4294, -0.1290, -0.0290]],\n",
       "\n",
       "        [[-0.4155, -0.1122, -0.0429]],\n",
       "\n",
       "        [[-0.4257, -0.1378, -0.0455]],\n",
       "\n",
       "        [[-0.4045, -0.1356, -0.0506]],\n",
       "\n",
       "        [[-0.4172, -0.1104, -0.0393]],\n",
       "\n",
       "        [[-0.4124, -0.1381, -0.0513]],\n",
       "\n",
       "        [[-0.4249, -0.1271, -0.0460]],\n",
       "\n",
       "        [[-0.4201, -0.1232, -0.0415]],\n",
       "\n",
       "        [[-0.4244, -0.1257, -0.0473]],\n",
       "\n",
       "        [[-0.4323, -0.1394, -0.0400]],\n",
       "\n",
       "        [[-0.4294, -0.1132, -0.0420]],\n",
       "\n",
       "        [[-0.4301, -0.1235, -0.0421]],\n",
       "\n",
       "        [[-0.4328, -0.1327, -0.0333]],\n",
       "\n",
       "        [[-0.4377, -0.1278, -0.0388]],\n",
       "\n",
       "        [[-0.4387, -0.1147, -0.0402]],\n",
       "\n",
       "        [[-0.4325, -0.1259, -0.0268]],\n",
       "\n",
       "        [[-0.4136, -0.1354, -0.0346]],\n",
       "\n",
       "        [[-0.4315, -0.1170, -0.0331]],\n",
       "\n",
       "        [[-0.4364, -0.1181, -0.0607]],\n",
       "\n",
       "        [[-0.4320, -0.1177, -0.0315]],\n",
       "\n",
       "        [[-0.4352, -0.1089, -0.0397]],\n",
       "\n",
       "        [[-0.4259, -0.1356, -0.0440]],\n",
       "\n",
       "        [[-0.4514, -0.1266, -0.0425]],\n",
       "\n",
       "        [[-0.4123, -0.1271, -0.0541]],\n",
       "\n",
       "        [[-0.4213, -0.1180, -0.0289]],\n",
       "\n",
       "        [[-0.4388, -0.1199, -0.0641]],\n",
       "\n",
       "        [[-0.4163, -0.1285, -0.0389]],\n",
       "\n",
       "        [[-0.4296, -0.1300, -0.0359]],\n",
       "\n",
       "        [[-0.4248, -0.1063, -0.0490]],\n",
       "\n",
       "        [[-0.4330, -0.1225, -0.0499]],\n",
       "\n",
       "        [[-0.4267, -0.1307, -0.0459]],\n",
       "\n",
       "        [[-0.4375, -0.1287, -0.0433]],\n",
       "\n",
       "        [[-0.4246, -0.1362, -0.0475]],\n",
       "\n",
       "        [[-0.4328, -0.1172, -0.0465]],\n",
       "\n",
       "        [[-0.4153, -0.1320, -0.0329]],\n",
       "\n",
       "        [[-0.4360, -0.1320, -0.0551]],\n",
       "\n",
       "        [[-0.4243, -0.1151, -0.0568]],\n",
       "\n",
       "        [[-0.4157, -0.1221, -0.0406]],\n",
       "\n",
       "        [[-0.4212, -0.1181, -0.0472]],\n",
       "\n",
       "        [[-0.4132, -0.1349, -0.0530]],\n",
       "\n",
       "        [[-0.4273, -0.1265, -0.0583]],\n",
       "\n",
       "        [[-0.4204, -0.1418, -0.0575]],\n",
       "\n",
       "        [[-0.4348, -0.1261, -0.0315]],\n",
       "\n",
       "        [[-0.4334, -0.1570, -0.0352]],\n",
       "\n",
       "        [[-0.4297, -0.1185, -0.0480]],\n",
       "\n",
       "        [[-0.4160, -0.1051, -0.0438]],\n",
       "\n",
       "        [[-0.4228, -0.1295, -0.0432]],\n",
       "\n",
       "        [[-0.4195, -0.1319, -0.0347]],\n",
       "\n",
       "        [[-0.4240, -0.1296, -0.0546]],\n",
       "\n",
       "        [[-0.4253, -0.1324, -0.0374]],\n",
       "\n",
       "        [[-0.4045, -0.1205, -0.0424]],\n",
       "\n",
       "        [[-0.4251, -0.1283, -0.0486]],\n",
       "\n",
       "        [[-0.4173, -0.1395, -0.0293]],\n",
       "\n",
       "        [[-0.4272, -0.1423, -0.0231]],\n",
       "\n",
       "        [[-0.4354, -0.1318, -0.0501]],\n",
       "\n",
       "        [[-0.4322, -0.1254, -0.0477]],\n",
       "\n",
       "        [[-0.4236, -0.1300, -0.0532]],\n",
       "\n",
       "        [[-0.4318, -0.1396, -0.0374]],\n",
       "\n",
       "        [[-0.4076, -0.1375, -0.0376]],\n",
       "\n",
       "        [[-0.4141, -0.1052, -0.0254]],\n",
       "\n",
       "        [[-0.4160, -0.1202, -0.0389]],\n",
       "\n",
       "        [[-0.4320, -0.1233, -0.0435]],\n",
       "\n",
       "        [[-0.4136, -0.1314, -0.0333]],\n",
       "\n",
       "        [[-0.4432, -0.1263, -0.0582]],\n",
       "\n",
       "        [[-0.4211, -0.1125, -0.0328]],\n",
       "\n",
       "        [[-0.4247, -0.1404, -0.0396]],\n",
       "\n",
       "        [[-0.4105, -0.1355, -0.0438]]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_delta_time = 1.0\n",
    "max_delta_time = 5.0\n",
    "max_points_num = 32\n",
    "\n",
    "\n",
    "def create_batch():\n",
    "    t0 = np.random.uniform(0, t_max - max_delta_time)\n",
    "    t1 = t0 + np.random.uniform(min_delta_time, max_delta_time)\n",
    "\n",
    "    idx = sorted(np.random.permutation(index_np[(times_np > t0) & (times_np < t1)])[:max_points_num])\n",
    "\n",
    "    obs_ = obs[idx]\n",
    "    ts_ = times[idx]\n",
    "    return obs_, ts_\n",
    "\n",
    "    # Train Neural ODE\n",
    "optimizer = torch.optim.Adam(ode_trained.parameters(), lr=0.01)\n",
    "train_losses = []\n",
    "\n",
    "obs_, ts_ = create_batch()\n",
    "z_ = ode_trained(obs_[0], U, ts_, return_whole_sequence=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
